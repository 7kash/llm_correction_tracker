"""
LLM Inference Tracker - Gradio App

Track how LLMs change their responses when corrected, with visualizations
of internal mechanisms: attention, softmax, and layer-by-layer predictions.

Deployment: Hugging Face Spaces
"""

import html
import math
import re
import time

import gradio as gr
from typing import List, Tuple, Dict, Optional

from backend.llm_with_internals import LLMWithInternals
from visualizations.answer_flow import get_clean_words, clean_token


# Global model instance (loaded once)
MODEL = None
DEFAULT_TEMPERATURE = 0.2


def load_model():
    """Load model on first use (lazy loading)."""
    global MODEL
    if MODEL is None:
        print("üîß Loading TinyLlama model...")
        MODEL = LLMWithInternals(model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0")
        print("‚úÖ Model ready!")
    return MODEL


def _format_percentage_bar(
    label: str, value: float, color_var: str, secondary_label: str = ""
) -> str:
    """Create a horizontal bar for percentage-based metrics."""
    safe_label = html.escape(label)
    safe_secondary = html.escape(secondary_label) if secondary_label else "&nbsp;"
    display_pct = round(value, 1)
    safe_value = max(0.0, value)
    bar_width = min(100, int(round(safe_value)))
    secondary_html = f"<span class=\"metric-secondary\">{safe_secondary}</span>"
    return "".join(
        [
            "<div class=\"metric-row\">",
            f"<span class=\"metric-label\">{safe_label}</span>",
            secondary_html,
            "<div class=\"metric-track\">",
            f"<div class=\"metric-fill\" style=\"width: {bar_width}%; background: var({color_var});\"></div>",
            "</div>",
            f"<span class=\"metric-value\">{display_pct}%</span>",
            "</div>",
        ]
    )


def _prepare_prompt_word_rows(internals: dict) -> Tuple[List[dict], str, Optional[str]]:
    """Return ordered prompt/context words aligned with their token indices."""

    def split_words(text: str) -> List[str]:
        if not text:
            return []
        return [word for word in re.split(r"[^0-9A-Za-z']+", text) if word]

    def normalise_word(text: str) -> str:
        return re.sub(r"[^0-9A-Za-z']+", "", (text or "")).lower()

    question = internals.get("question", "")
    context = internals.get("context", "")
    prompt_html = "<div class=\"prompt-summary\">"
    prompt_html += f"<strong>Prompt:</strong> {html.escape(question) if question else '‚Äî'}"
    if context:
        prompt_html += f"<br><strong>Context:</strong> {html.escape(context)}"
    prompt_html += "</div>"

    if not question and not context:
        return [], prompt_html, "_Provide a prompt to see attention percentages._"

    ordered_words: List[dict] = []
    for source, raw_text in (("Prompt", question), ("Context", context)):
        for word in split_words(raw_text):
            ordered_words.append(
                {
                    "label": word,
                    "key": normalise_word(word),
                    "source": source,
                }
            )

    if not ordered_words:
        return [], prompt_html, "_No prompt words were captured for attention tracking._"

    token_assignments: List[List[int]] = [None] * len(ordered_words)  # type: ignore
    prompt_tokens = internals.get("input_tokens", [])
    cleaned_tokens = list(get_clean_words(prompt_tokens))

    for word, token_indices in cleaned_tokens:
        key = normalise_word(word)
        if not key:
            continue
        for idx, entry in enumerate(ordered_words):
            if entry["key"] != key:
                continue
            if token_assignments[idx] is None:
                token_assignments[idx] = list(token_indices)
                break

    rows: List[dict] = []
    for idx, entry in enumerate(ordered_words):
        rows.append(
            {
                "order": idx,
                "label": entry["label"],
                "source": entry["source"],
                "token_indices": token_assignments[idx] or [],
            }
        )

    return rows, prompt_html, None


def _collect_attention_metrics(internals: dict) -> Tuple[List[Tuple[str, float]], str]:
    """Return attention percentages for every prompt/context word with formatted HTML."""

    heatmap = internals.get("attention_heatmap") or {}
    prompt_tokens = heatmap.get("prompt_tokens") or internals.get("input_tokens", [])

    rows, prompt_html, warning = _prepare_prompt_word_rows({**internals, "input_tokens": prompt_tokens})
    if warning:
        return [], prompt_html + warning

    values = heatmap.get("values") or []
    attention_percentages = internals.get("attention_percentages")

    metrics: List[Tuple[str, float]] = []
    metric_rows: List[str] = []
    seen_indices = set()

    def add_metric(label: str, total: float, source: str):
        metrics.append((label, total))
        metric_rows.append(
            _format_percentage_bar(label, total, "--attention-color", source)
        )

    source_column: List[float] = []
    if values:
        source_column = [float(x) for x in values[0]]

    if source_column:
        for row in rows:
            total_attention = 0.0
            for idx in row["token_indices"]:
                if idx < len(source_column):
                    total_attention += source_column[idx]
                    seen_indices.add(idx)
            add_metric(row["label"], total_attention, row["source"])

        remainder = sum(source_column[idx] for idx in range(len(source_column)) if idx not in seen_indices)
        if remainder > 0.1:
            add_metric("Other prompt tokens", remainder, "Prompt")
    elif attention_percentages:
        for row in rows:
            total_attention = 0.0
            for idx in row["token_indices"]:
                if idx < len(attention_percentages):
                    total_attention += float(attention_percentages[idx])
                    seen_indices.add(idx)
            add_metric(row["label"], total_attention, row["source"])

        remainder = sum(
            float(attention_percentages[idx])
            for idx in range(len(attention_percentages))
            if idx not in seen_indices
        )
        if remainder > 0.1:
            add_metric("Other prompt tokens", remainder, "Prompt")
    else:
        return [], prompt_html + "_Attention data is not available for this turn yet._"

    metrics_html = "<div class=\"metric-list\">" + "".join(metric_rows) + "</div>"
    return metrics, prompt_html + metrics_html


def _collect_softmax_metrics(internals: dict) -> Tuple[List[dict], str]:
    """Return softmax token candidates and formatted HTML."""
    softmax_example = internals.get("softmax_example")
    if not softmax_example:
        return [], "_Probability data will appear once the model proposes a token._"

    items = []
    rows = []
    for item in softmax_example:
        token = item.get("token", "?")
        prob = float(item.get("probability", 0.0)) * 100
        raw_logit = item.get("logit")
        logit = float(raw_logit) if raw_logit is not None else None
        cleaned = clean_token(token)
        if cleaned:
            token_display, _ = cleaned
        else:
            token_display = token.replace("‚ñÅ", " ").strip() or token

        items.append({
            "token": token_display,
            "prob": prob,
            "logit": logit,
        })
        bar_width = min(100, int(round(prob)))
        logit_text = f"logit {logit:+.2f}" if logit is not None else "‚Äî"
        rows.append(
            "<div class=\"metric-row\">"
            f"<span class=\"metric-label\">{token_display}</span>"
            f"<span class=\"metric-logit\">{logit_text}</span>"
            "<div class=\"metric-track\">"
            f"<div class=\"metric-fill\" style=\"width: {bar_width}%; background: var(--softmax-color);\"></div>"
            "</div>"
            f"<span class=\"metric-value\">{prob:.1f}%</span>"
            "</div>"
        )

    total_prob = sum(entry["prob"] for entry in items)
    remainder = max(0.0, 100.0 - total_prob)
    if remainder > 0.05:
        bar_width = min(100, int(round(remainder)))
        items.append({
            "token": "Other tokens",
            "prob": remainder,
            "logit": None,
        })
        rows.append(
            "<div class=\"metric-row\">"
            "<span class=\"metric-label\">Other tokens</span>"
            "<span class=\"metric-logit\">‚Äî</span>"
            "<div class=\"metric-track\">"
            f"<div class=\"metric-fill\" style=\"width: {bar_width}%; background: var(--softmax-color);\"></div>"
            "</div>"
            f"<span class=\"metric-value\">{remainder:.1f}%</span>"
            "</div>"
        )

    return items, "<div class=\"metric-list\">" + "".join(rows) + "</div>"


def _collect_layer_metrics(internals: dict, actual_answer: str) -> Tuple[List[dict], str]:
    """Return layer-by-layer predictions and formatted HTML."""
    layer_predictions = internals.get("layer_predictions") or []
    if not layer_predictions:
        return [], "_Layer probes activate once the model begins forming an answer._"

    layer_rows = []
    metrics = []
    for layer_idx, layer_data in enumerate(layer_predictions):
        predictions = layer_data.get("predictions", [])
        actual_entry = None
        alternatives = []

        for pred in predictions:
            token = pred.get("token", "")
            prob = float(pred.get("probability", 0.0)) * 100
            is_actual = pred.get("is_actual_answer", False)
            cleaned = clean_token(token)
            if cleaned:
                display_token, _ = cleaned
            else:
                display_token = token.replace("‚ñÅ", " ").strip() or token

            data_point = {"token": display_token, "prob": prob}
            if is_actual:
                actual_entry = data_point
            else:
                alternatives.append(data_point)

        alternatives = alternatives[:3]
        metrics.append({
            "layer": layer_idx,
            "actual": actual_entry,
            "alternatives": alternatives,
        })

        alt_lines = "".join(
            f"<li>{alt['token']} ‚Äî {alt['prob']:.1f}%</li>" for alt in alternatives
        ) or "<li>No strong alternatives</li>"

        actual_line = (
            f"<strong>{actual_entry['token']}</strong> ‚Äî {actual_entry['prob']:.1f}%"
            if actual_entry
            else "Model had not locked onto the final word yet"
        )

        layer_rows.append(
            "<div class=\"layer-card\">"
            f"<div class=\"layer-title\">Layer {layer_idx}</div>"
            f"<div class=\"layer-actual\">{actual_line}</div>"
            f"<ul class=\"layer-alternatives\">{alt_lines}</ul>"
            "</div>"
        )

    return metrics, "<div class=\"layer-grid\">" + "".join(layer_rows) + "</div>"


def _display_token(token: str) -> str:
    cleaned = clean_token(token or "")
    if cleaned:
        token_text, _ = cleaned
        token_text = token_text.strip()
        if token_text:
            return html.escape(token_text)
    safe = (token or "").replace("‚ñÅ", " ").strip()
    return html.escape(safe or token or "‚àÖ")


def _render_attention_heatmap(heatmap: dict, internals: dict) -> str:
    if not heatmap:
        return "<em>Run the model to see a token-by-token attention heatmap.</em>"

    raw_output_tokens = heatmap.get("output_tokens") or []
    values = heatmap.get("values") or []

    if not raw_output_tokens or not values:
        return "<em>Attention heatmap data is unavailable.</em>"

    matched_rows, _, warning = _prepare_prompt_word_rows(
        {
            **internals,
            "input_tokens": heatmap.get("prompt_tokens") or [],
        }
    )

    if warning and not matched_rows:
        return warning.replace("attention percentages", "attention heatmap")

    if not matched_rows:
        return "<em>No prompt or context words were detected for the heatmap.</em>"

    matched_rows.sort(key=lambda item: item["order"])

    # Combine output tokens into readable words.
    output_word_entries = []
    output_words = get_clean_words(raw_output_tokens)
    if output_words:
        for idx, (word, token_indices) in enumerate(output_words):
            output_word_entries.append(
                {
                    "label": word,
                    "indices": token_indices,
                    "order": idx,
                }
            )
    else:
        for idx, token in enumerate(raw_output_tokens):
            output_word_entries.append(
                {
                    "label": (_display_token(token) or f"Token {idx+1}"),
                    "indices": [idx],
                    "order": idx,
                }
            )

    if not output_word_entries or not values:
        return "<em>Attention heatmap data is unavailable.</em>"

    header_cells = []
    for entry in output_word_entries:
        header_cells.append(f"<th>{html.escape(entry['label'])}</th>")

    rows_html = []
    for row in matched_rows:
        cells = []
        for column in output_word_entries:
            pct = 0.0
            for output_idx in column["indices"]:
                if output_idx >= len(values):
                    continue
                column_values = values[output_idx]
                if not column_values:
                    continue
                if row["token_indices"]:
                    for prompt_idx in row["token_indices"]:
                        if prompt_idx < len(column_values):
                            pct += float(column_values[prompt_idx])
                else:
                    # Row without explicit tokens still appears but registers zero attention.
                    pct += 0.0
            intensity = min(0.85, 0.15 + (pct / 100.0) * 0.7)
            cells.append(
                "<td style=\"background: rgba(249, 115, 22, {intensity:.3f});\">"
                f"<span class=\"heatmap-value\">{pct:.1f}%</span>"
                "</td>".format(intensity=intensity)
            )

        source_badge = f"<span class=\"heatmap-source heatmap-source-{row['source'].lower()}\">{row['source']}</span>"
        rows_html.append(
            "<tr>"
            f"<th scope=\"row\">{source_badge}{html.escape(row['label'])}</th>"
            + "".join(cells)
            + "</tr>"
        )

    header_html = "<tr><th>Prompt &amp; context words</th>" + "".join(header_cells) + "</tr>"

    return (
        "<div class=\"heatmap-wrapper\">"
        "<div class=\"heatmap-heading\"><strong>Attention heatmap</strong> ‚Äî each cell shows how strongly an output word re-read your prompt or context terms.</div>"
        "<table class=\"heatmap-table\">"
        + header_html
        + "".join(rows_html)
        + "</table>"
        "</div>"
    )


def _render_softmax_waterfall(metrics: List[dict], temperature: Optional[float] = None) -> str:
    if not metrics:
        return "<em>Softmax waterfall will appear after the model proposes a token.</em>"

    logit_entries = [m for m in metrics if m.get("logit") is not None]
    if not logit_entries:
        return "<em>Logit data was unavailable for this token.</em>"

    max_logit = max(m["logit"] for m in logit_entries)
    sum_shifted_all = None
    shifted_values = {}

    for entry in logit_entries:
        prob = max(entry.get("prob", 0.0) / 100.0, 1e-9)
        shift = entry["logit"] - max_logit
        exp_shift = math.exp(shift)
        shifted_values[entry["token"]] = exp_shift
        if sum_shifted_all is None and prob > 0:
            sum_shifted_all = exp_shift / prob

    if sum_shifted_all is None:
        sum_shifted_all = sum(shifted_values.values()) or 1.0

    rows = []
    for entry in metrics:
        token = entry.get("token", "?")
        prob_pct = float(entry.get("prob", 0.0))
        prob_fraction = prob_pct / 100.0

        if entry.get("logit") is not None:
            exp_shift = shifted_values.get(token, 0.0)
        else:
            exp_shift = prob_fraction * sum_shifted_all

        exp_width = min(100, int(round((exp_shift / sum_shifted_all) * 100))) if sum_shifted_all else 0
        prob_width = min(100, int(round(prob_pct)))
        logit_text = f"{entry['logit']:+.2f} logit" if entry.get("logit") is not None else "‚Äî"

        rows.append(
            "<div class=\"waterfall-row\">"
            f"<div class=\"waterfall-token\">{html.escape(token)}</div>"
            f"<div class=\"waterfall-stage\">{logit_text}</div>"
            f"<div class=\"waterfall-stage\"><div class=\"waterfall-bar exp\" style=\"width:{exp_width}%;\"></div><span>exp= {exp_shift:.3f}</span></div>"
            f"<div class=\"waterfall-stage\"><div class=\"waterfall-bar prob\" style=\"width:{prob_width}%;\"></div><span>{prob_pct:.1f}%</span></div>"
            "</div>"
        )

    if temperature is not None:
        explainer = (
            "<div class=\"waterfall-heading\"><strong>Softmax waterfall</strong> ‚Äî logits are scaled by the temperature "
            f"(<code>{temperature:.2f}</code>) before exponentiation. Higher temperatures flatten the exp column; lower ones make the winning bar sharper.</div>"
        )
    else:
        explainer = (
            "<div class=\"waterfall-heading\"><strong>Softmax waterfall</strong> ‚Äî track each token from raw logit through exponentiation to the final probability.</div>"
        )

    explainer += (
        "<p class=\"waterfall-note\">Read the columns left to right: the first shows the raw logit (after temperature), the middle shows the exponentiated value before normalisation, and the final purple bar is the true probability that feeds sampling.</p>"
    )

    return (
        "<div class=\"waterfall-wrapper\">"
        + explainer
        + "".join(rows)
        + "</div>"
    )


def _render_layer_sparkline(metrics: List[dict], generated_tokens: List[str]) -> str:
    if not metrics:
        return "<em>Run the model to chart confidence growth across layers.</em>"

    confidences = [max(0.0, min(100.0, (entry.get("actual") or {}).get("prob", 0.0))) for entry in metrics]
    if not any(confidences):
        return "<em>Confidence sparkline will appear once the model locks onto a candidate.</em>"

    width = 360
    height = 120
    padding = 24
    step = (width - 2 * padding) / max(1, len(confidences) - 1)
    points = []
    for idx, prob in enumerate(confidences):
        x = padding + step * idx
        y = height - padding - (prob / 100.0) * (height - 2 * padding)
        points.append(f"{x:.1f},{y:.1f}")

    area_points = " ".join([f"{padding},{height - padding}"] + points + [f"{padding + step * (len(confidences) - 1):.1f},{height - padding}"])
    polyline_points = " ".join(points)
    final_token = generated_tokens[0] if generated_tokens else "the answer"
    axis_y = height - padding
    axis_end = padding + step * max(1, len(confidences) - 1)
    tick_marks = []
    for idx in range(len(confidences)):
        x = padding + step * idx
        tick_marks.append(
            f"<line x1='{x:.1f}' y1='{axis_y}' x2='{x:.1f}' y2='{axis_y + 6}' class='sparkline-tick-mark' />"
        )
        tick_marks.append(
            f"<text x='{x:.1f}' y='{axis_y + 18}' class='sparkline-tick-label'>{idx}</text>"
        )
    ticks_svg = "".join(tick_marks)

    return (
        "<div class=\"sparkline-wrapper\">"
        "<div class=\"sparkline-heading\"><strong>Confidence sparkline</strong> ‚Äî see how the model warmed up to"
        f" {_display_token(final_token)}.</div>"
        f"<svg viewBox=\"0 0 {width} {height}\" class=\"sparkline\">"
        f"<path d=\"M {area_points} Z\" class=\"sparkline-area\" />"
        f"<polyline points=\"{polyline_points}\" class=\"sparkline-line\" />"
        f"<line x1='{padding}' y1='{axis_y}' x2='{axis_end:.1f}' y2='{axis_y}' class=\"sparkline-axis-line\" />"
        f"<text x='{axis_end:.1f}' y='{axis_y + 32}' class=\"sparkline-axis-caption\">Layer ‚Üí</text>"
        f"{ticks_svg}"
        "</svg>"
        "</div>"
    )


def _render_feedback_delta(original_payload: dict, corrected_payload: dict) -> str:
    if not original_payload or not corrected_payload:
        return "<em>Generate a correction to compare attention, probabilities, and layers.</em>"

    def build_lookup(metrics):
        lookup = {}
        if not metrics:
            return lookup
        for item in metrics:
            if isinstance(item, tuple) and len(item) == 2:
                label, value = item
            elif isinstance(item, dict):
                label = item.get("token")
                value = item.get("prob", 0.0)
            else:
                continue
            key = (label or "").strip().lower()
            if not key:
                continue
            lookup[key] = {"label": label, "value": float(value)}
        return lookup

    orig_attn = build_lookup(original_payload.get("attention", {}).get("metrics"))
    corr_attn = build_lookup(corrected_payload.get("attention", {}).get("metrics"))
    attn_items = []
    for key in set(orig_attn) | set(corr_attn):
        before = orig_attn.get(key, {"label": corr_attn.get(key, {}).get("label", key), "value": 0.0})
        after = corr_attn.get(key, {"label": before.get("label", key), "value": 0.0})
        delta = after["value"] - before["value"]
        attn_items.append({"label": after.get("label") or before.get("label") or key, "delta": delta})

    attn_items = [item for item in attn_items if abs(item["delta"]) > 0.1]
    attn_items.sort(key=lambda item: abs(item["delta"]), reverse=True)
    attn_html = "".join(
        f"<li><span class=\"delta-label\">{html.escape(item['label'])}</span><span class=\"delta-value\">{item['delta']:+.1f} pts</span></li>"
        for item in attn_items[:4]
    ) or "<li>No major shifts detected.</li>"

    orig_softmax = build_lookup(original_payload.get("softmax", {}).get("metrics"))
    corr_softmax = build_lookup(corrected_payload.get("softmax", {}).get("metrics"))
    softmax_items = []
    for key in set(orig_softmax) | set(corr_softmax):
        before = orig_softmax.get(key, {"label": corr_softmax.get(key, {}).get("label", key), "value": 0.0})
        after = corr_softmax.get(key, {"label": before.get("label", key), "value": 0.0})
        delta = after["value"] - before["value"]
        softmax_items.append({"label": after.get("label") or before.get("label") or key, "delta": delta})

    softmax_items = [item for item in softmax_items if abs(item["delta"]) > 0.05]
    softmax_items.sort(key=lambda item: abs(item["delta"]), reverse=True)
    softmax_html = "".join(
        f"<li><span class=\"delta-label\">{html.escape(item['label'])}</span><span class=\"delta-value\">{item['delta']:+.1f} pts</span></li>"
        for item in softmax_items[:4]
    ) or "<li>Probability slate stayed steady.</li>"

    orig_layers = original_payload.get("layers", {}).get("metrics", [])
    corr_layers = corrected_payload.get("layers", {}).get("metrics", [])
    layer_changes = []
    for idx in range(max(len(orig_layers), len(corr_layers))):
        orig_entry = orig_layers[idx].get("actual") if idx < len(orig_layers) and isinstance(orig_layers[idx], dict) else None
        corr_entry = corr_layers[idx].get("actual") if idx < len(corr_layers) and isinstance(corr_layers[idx], dict) else None
        before = (orig_entry or {}).get("prob", 0.0)
        after = (corr_entry or {}).get("prob", 0.0)
        delta = after - before
        if abs(delta) < 0.1:
            continue
        layer_changes.append({"layer": idx, "delta": delta})

    layer_changes.sort(key=lambda item: abs(item["delta"]), reverse=True)
    layer_html = "".join(
        f"<li>Layer {item['layer']}: {item['delta']:+.1f} pts toward the answer</li>"
        for item in layer_changes[:4]
    ) or "<li>Layer confidence barely moved.</li>"

    return (
        "<div class=\"delta-wrapper\">"
        "<div class=\"delta-heading\"><strong>Feedback impact</strong> ‚Äî quick view of what changed most.</div>"
        "<div class=\"delta-grid\">"
        f"<div class=\"delta-card\"><h4>Attention shifts</h4><ul>{attn_html}</ul></div>"
        f"<div class=\"delta-card\"><h4>Probability swings</h4><ul>{softmax_html}</ul></div>"
        f"<div class=\"delta-card\"><h4>Layer momentum</h4><ul>{layer_html}</ul></div>"
        "</div>"
        "</div>"
    )


PIPELINE_STEPS = [
    (
        "Tokenise prompt",
        "Split the question and any correction into tokens TinyLlama can read.",
    ),
    (
        "Share context via attention",
        "Each token looks back over the prompt to borrow the most relevant words.",
    ),
    (
        "Refine through transformer layers",
        "Feed-forward blocks reshape the focused context at every depth.",
    ),
    (
        "Score candidates",
        "The prediction head produces a logit score for every vocabulary option.",
    ),
    (
        "Sample the next token",
        "Softmax (plus the temperature slider) turns scores into the chosen answer.",
    ),
]


def _build_pipeline_html(
    completed_steps: int = 0,
    active_step: Optional[int] = None,
    temperature: Optional[float] = None,
) -> str:
    steps_html = []
    for index, (title, description) in enumerate(PIPELINE_STEPS, start=1):
        if index <= completed_steps:
            status_class = "pipeline-step-done"
            icon = "‚úì"
        elif active_step is not None and index == active_step:
            status_class = "pipeline-step-active"
            icon = "‚Ä¶"
        else:
            status_class = "pipeline-step-pending"
            icon = ""

        steps_html.append(
            "<li class=\"pipeline-step {status}\">".format(status=status_class)
            + f"<span class=\"pipeline-check\">{icon}</span>"
            + "<div class=\"pipeline-text\">"
            + f"<span class=\"pipeline-step-label\">{html.escape(title)}</span>"
            + f"<p>{html.escape(description)}</p>"
            + "</div>"
            + "</li>"
        )

    temperature_note = (
        f"<p class=\"pipeline-temperature\">Temperature: <strong>{temperature:.2f}</strong> ‚Äî lower keeps answers deterministic, higher spreads probability mass.</p>"
        if temperature is not None
        else ""
    )

    return (
        "<div class=\"pipeline-card\">"
        "<h4>Transformer pipeline checklist</h4>"
        "<ol>"
        + "".join(steps_html)
        + "</ol>"
        + temperature_note
        + "</div>"
    )


THEORY_TEXT = {
    "overview": """
### Transformers in plain language

Large language models are enormous autocomplete engines. They slice your words into **tokens**, turn each token into numbers, and then predict which token should come next. Transformers are the clever architecture behind this trick: they let every token peek at the rest of the sentence, refine that shared context layer after layer, and finally choose the most promising next word.

**Why TinyLlama?** TinyLlama-1.1B is a small, open-source chat model with around 1.1‚ÄØbillion learned parameters. ChatGPT‚Äôs GPT-4 uses tens of billions, so GPT-4 is far more capable, but also far more opaque. TinyLlama‚Äôs modest size lets us inspect the exact attention weights, intermediate probabilities, and layer outputs without specialised tooling.

#### Follow the five stages
1. **Tokenise and embed.** Your question (plus any correction) is broken into tokens and turned into vectors that encode position and meaning.
2. **Share context with attention.** Each token scores how relevant every other token is, so the model can ‚Äúre-read‚Äù the important words before it answers.
3. **Refine through transformer layers.** Twenty-two stacked layers repeatedly mix attention with tiny neural networks to polish the representation.
4. **Score every candidate word.** A prediction head compares the final vector against the vocabulary to produce a logit score for every possible next token.
5. **Sample using softmax.** The softmax function (shaped by the Temperature slider) turns those scores into probabilities and selects the next token.

The blue checklist beside the title runs through these steps in real time. The Query tab mirrors them: orange attention metrics for step‚ÄØ2, purple softmax panels for steps‚ÄØ4‚Äì5, and blue layer probes for step‚ÄØ3.

_Try it:_ ask **What is the capital of Australia?** Run it once at temperature 0.2 and again at 1.0. You‚Äôll see the checklist tick through the stages while the attention and probability panels show how cooler or warmer sampling changes the decision.
""".strip(),
    "feedback": """
### What happens when you give feedback

The model never rewires itself here, but each new turn replays the conversation with your hint inserted at the top. That means the entire pipeline‚Äîattention, layers, and softmax‚Äîruns again while treating your guidance as part of the prompt.

1. **That‚Äôs Wrong.** We prepend a short reminder that the last answer missed the mark and ask TinyLlama to try again.
2. **Submit Correction.** We add the exact word you provided so the model can focus all its attention, layer updates, and probabilities on that target.

On the second pass you can watch: 
* **Attention shifts.** Words that echo your hint glow brighter because the model keeps rereading them.
* **Layer adjustments.** The corrected answer usually becomes confident earlier in the stack.
* **Probability swings.** Softmax reallocates probability mass away from old guesses toward the corrected token.

The Œî Impact tab condenses those differences so you can compare ‚Äúbefore‚Äù and ‚Äúafter‚Äù without scrolling.

_Try it:_ run a question, press **That‚Äôs Wrong**, and check how the attention bars change. Then supply the right word and press **Submit Correction** to see the layers and probabilities snap toward your goal.
""".strip(),
    "attention": """
### How attention works

Attention is the model‚Äôs spotlight. Every token asks, ‚ÄúWhich earlier words should I reread before I speak?‚Äù

1. It builds three vectors for each token: a **query** (what I need), a **key** (how much of that I contain), and a **value** (the information I can hand over).
2. The query compares itself with every key. After a quick scaling step, softmax turns those similarity scores into clear percentages that sum to 100.
3. The token then blends the value vectors using those percentages, creating a custom summary of the prompt to guide the next prediction.

The orange list shows the total percentage each prompt or context word contributed when TinyLlama chose the highlighted answer. The heatmap underneath breaks the same data down by generated token so you can trace exactly where the model looked for evidence.

_Try it:_ hover over a heatmap column to see which prompt word dominated that token, then find the same word‚Äôs overall percentage in the bar chart.
""".strip(),
    "softmax": """
### How softmax turns scores into probabilities

By the time attention and the layers finish, TinyLlama has a **logit** score for every word in its vocabulary. Softmax is the final translator that turns those raw scores into real probabilities.

1. **Adjust for temperature.** We divide each logit by the Temperature slider. Cooler temperatures (<1) exaggerate differences so the top word dominates; warmer temperatures (>1) smooth things out and keep more options alive.
2. **Exponentiate.** Each adjusted logit goes through $e^{z}$, which blows up high scores and shrinks low ones.
3. **Normalise.** We divide by the sum of all exponentials so the probabilities add up to 100%.

The purple list shows the final probabilities for the leading candidates. The waterfall diagram follows those same tokens through the three stages so you can see where one option starts to sprint ahead. The ‚ÄúOther tokens‚Äù bar gathers the tail of the distribution so nothing is missing.

_Try it:_ run a question at temperature 0.2 and again at 1.0. Watch the middle ‚Äúexp‚Äù column: when the temperature is higher the bars are closer together, and the final probabilities spread out accordingly.
""".strip(),
    "layers": """
### Why layer-by-layer probes matter

Transformers don‚Äôt make a decision in one jump. Each layer nudges the representation a little closer to the final answer. By attaching the prediction head to every layer‚Äîa technique nicknamed the **logit lens**‚Äîwe can see how confident the model would be if it stopped early.

* **Early layers** mostly identify the topic, so the right word might barely lead.
* **Middle layers** mix in attention results and start ruling out bad candidates.
* **Late layers** push the winning token far ahead, making the probability curve shoot toward 100%.

The blue cards list the top competitors at each layer, and the sparkline plots how the probability of the actual answer rises as the layer number increases.

_Try it:_ after giving feedback, check whether the sparkline climbs sooner. A steeper early rise means the model locked onto your hint earlier in the stack.
""".strip(),
}


def build_visualization_payload(internals: dict, guidance_note: str = "") -> dict:
    """Bundle friendly explanations, metrics, and theory snippets for the UI."""

    question = internals.get("question", "").strip() or "your question"
    answer = internals.get("response", "").strip() or internals.get("text", "")
    temperature = internals.get("temperature")

    overview_lines = [
        "### How the model responded",
        f"It read **{html.escape(question)}** and proposed **{html.escape(answer or '...')}** as its next word.",
        "The tabs below unpack where it placed attention, how likely each candidate word looked, and how certainty grew across layers.",
    ]
    if temperature is not None:
        if temperature <= 0.35:
            temp_descriptor = "a sharp, deterministic slate"
        elif temperature >= 0.9:
            temp_descriptor = "a very exploratory probability slate"
        else:
            temp_descriptor = "a balanced probability slate"
        overview_lines.append(
            f"Temperature was set to **{temperature:.2f}**, so expect {temp_descriptor}."
        )
    if guidance_note:
        overview_lines.append(f"<div class='soft-note'>{guidance_note}</div>")

    attention_metrics, attention_html = _collect_attention_metrics(internals)
    softmax_metrics, softmax_html = _collect_softmax_metrics(internals)
    layer_metrics, layer_html = _collect_layer_metrics(internals, answer)
    attention_heatmap_html = _render_attention_heatmap(internals.get("attention_heatmap"), internals)
    softmax_waterfall_html = _render_softmax_waterfall(
        softmax_metrics,
        internals.get("temperature"),
    )
    layer_sparkline_html = _render_layer_sparkline(layer_metrics, internals.get("generated_tokens", []))

    return {
        "question": question,
        "answer": answer,
        "overview": "\n\n".join(overview_lines),
        "attention": {
            "metrics": attention_metrics,
            "markdown": (
                "<div class=\"section-note pill-attention\"><strong>Where focus went:</strong> higher bars mean the model relied more on that word.</div>"
                + attention_html
            ),
            "heatmap": attention_heatmap_html,
        },
        "softmax": {
            "metrics": softmax_metrics,
            "markdown": (
                "<div class=\"section-note pill-softmax\"><strong>Top candidates:</strong> the probability shows how confident the model was before choosing.</div>"
                + softmax_html
            ),
            "waterfall": softmax_waterfall_html,
        },
        "layers": {
            "metrics": layer_metrics,
            "markdown": (
                "<div class=\"section-note pill-layers\"><strong>Confidence by layer:</strong> later layers should favour the final answer.</div>"
                + layer_html
            ),
            "sparkline": layer_sparkline_html,
        },
        "theory": {
            "overview": THEORY_TEXT["overview"],
            "feedback": THEORY_TEXT["feedback"],
            "attention": THEORY_TEXT["attention"],
            "softmax": THEORY_TEXT["softmax"],
            "layers": THEORY_TEXT["layers"],
        },
    }


def build_correction_sections(
    question: str,
    original_answer: str,
    original_payload: dict,
    correction_context: str,
    corrected_answer: str,
    corrected_payload: dict,
) -> Tuple[str, str, str, str, str]:
    """Return summary, per-section comparison HTML blocks, and delta view."""

    def strip_section_note(markup: str) -> str:
        if not markup:
            return "<em>No data available.</em>"
        if "section-note" not in markup:
            return markup
        closing = markup.find("</div>")
        if closing == -1:
            return markup
        trimmed = markup[closing + len("</div>"):].strip()
        return trimmed or "<em>No data available.</em>"

    def extract_markdown(payload: dict, key: str) -> str:
        if not isinstance(payload, dict):
            return "<em>No data available.</em>"
        section = payload.get(key, {})
        if not isinstance(section, dict):
            return "<em>No data available.</em>"
        markup = section.get("markdown", "")
        if not markup:
            return "<em>No data available.</em>"
        return strip_section_note(markup)

    safe_question = html.escape(question)
    safe_original = html.escape(original_answer or "‚Äî")
    safe_corrected = html.escape(corrected_answer or "‚Äî")
    safe_context = html.escape(correction_context)

    summary = f"""
<div class="section-note pill-layers">
    <strong>Question:</strong> {safe_question}<br>
    <strong>Before feedback:</strong> {safe_original}<br>
    <strong>After feedback:</strong> {safe_corrected}
</div>
<p style="font-size:0.9rem;color:#4B5563;">We reminded the model: <em>{safe_context}</em></p>
"""

    attention = f"""
<div class="comparison-card">
    <div class="column">
        <h4 style="margin-top:0; color: var(--attention-color);">Before feedback <span class="info-tooltip" data-tip="Attention percentages from the original prompt run.">?</span></h4>
        {extract_markdown(original_payload, 'attention')}
    </div>
    <div class="column">
        <h4 style="margin-top:0; color: var(--attention-color);">After feedback <span class="info-tooltip" data-tip="Attention recalculated after your correction was injected into the prompt.">?</span></h4>
        {extract_markdown(corrected_payload, 'attention')}
    </div>
</div>
"""

    softmax = f"""
<div class="comparison-card">
    <div class="column">
        <h4 style="margin-top:0; color: var(--softmax-color);">Before feedback <span class="info-tooltip" data-tip="Logits and probabilities the model considered before hearing the correction.">?</span></h4>
        {extract_markdown(original_payload, 'softmax')}
    </div>
    <div class="column">
        <h4 style="margin-top:0; color: var(--softmax-color);">After feedback <span class="info-tooltip" data-tip="New probability slate once the hint was added to the prompt.">?</span></h4>
        {extract_markdown(corrected_payload, 'softmax')}
    </div>
</div>
"""

    layers = f"""
<div class="comparison-card">
    <div class="column">
        <h4 style="margin-top:0; color: var(--layers-color);">Before feedback <span class="info-tooltip" data-tip="How confident each layer was in the original guess.">?</span></h4>
        {extract_markdown(original_payload, 'layers')}
    </div>
    <div class="column">
        <h4 style="margin-top:0; color: var(--layers-color);">After feedback <span class="info-tooltip" data-tip="Layer-by-layer view after the corrective hint.">?</span></h4>
        {extract_markdown(corrected_payload, 'layers')}
    </div>
</div>
"""

    delta = _render_feedback_delta(original_payload, corrected_payload)

    return summary, attention, softmax, layers, delta

def is_one_word_question(question: str) -> Tuple[bool, str]:
    """
    Check if question is likely answerable in one word.

    Returns
    -------
    (is_valid, message) : tuple
        is_valid: True if question looks one-word answerable
        message: Explanation if not valid
    """
    question_lower = question.lower().strip()

    # Check for common one-word question patterns
    one_word_patterns = [
        'what is',
        'who is',
        'where is',
        'when was',
        'when did',
        'which',
        'what was',
        'capital of',
        'color of',
        'color is',
        'president of',
        'author of',
        'founder of'
    ]

    # Check for multi-word answer indicators
    multi_word_indicators = [
        'why',
        'how does',
        'explain',
        'describe',
        'tell me about',
        'what are the'
    ]

    # If contains multi-word indicators, reject
    for indicator in multi_word_indicators:
        if indicator in question_lower:
            return False, f"Questions with '{indicator}' usually need more than one word. Try: 'What is the capital of Australia?'"

    # If contains one-word patterns, accept
    for pattern in one_word_patterns:
        if pattern in question_lower:
            return True, ""

    # Default: accept but warn
    return True, "Note: This works best with questions that have one-word answers (like 'What is X?' or 'Who is Y?')"


def generate_one_word_answer(question: str, context: str = None, temperature: float = DEFAULT_TEMPERATURE) -> Tuple[str, str]:
    """
    Generate one-word answer and show layer-by-layer predictions (logit lens).

    Parameters
    ----------
    question : str
        The question to answer
    context : str, optional
        Additional context to prepend (e.g., "The answer is Green")
    temperature : float, optional
        Softmax temperature applied before sampling/logit lens visualisations

    Returns
    -------
    answer : str
    visualization : str (markdown)
    """
    # Validate question (skip validation if context is provided, as it might change the format)
    if not context:
        is_valid, message = is_one_word_question(question)

        if not is_valid:
            error_viz = f"## ‚ö†Ô∏è Question Not Suitable\n\n{message}\n\n**Examples of good questions:**\n- What is the capital of Australia?\n- Who is the president of USA?\n- What color is the sky?\n"
            return "‚ùå Please ask a one-word answerable question", error_viz
    else:
        message = ""

    model = load_model()

    # Generate with layer-by-layer tracking
    result = model.generate_one_word_with_layers(
        question,
        max_new_tokens=10,
        context=context,
        temperature=temperature,
    )

    # Check if answer is actually one word (3 tokens max for compound words)
    answer_word_count = len(result["response"].split())
    if answer_word_count > 2:
        warning_viz = f"## ‚ö†Ô∏è Answer Too Long\n\nThe model generated: **{result['response']}** ({answer_word_count} words)\n\nThis visualization works best with single-word answers. Try rephrasing your question to expect a one-word answer.\n\n**Good examples:**\n- What is the capital of Australia?\n- Who discovered gravity?\n- What color is grass?"
        return result["response"], warning_viz

    # Create visualization
    payload = build_visualization_payload(result, guidance_note=message)

    return result["response"], payload


# ============================================================================
# Gradio Interface
# ============================================================================

def main_interface():
    # Minimalistic monochrome theme
    minimal_theme = gr.themes.Monochrome(
        primary_hue="slate",
        secondary_hue="slate",
        neutral_hue="slate",
        font=("system-ui", "sans-serif"),
    ).set(
        body_background_fill="white",
        button_primary_background_fill="#111827",
        button_primary_background_fill_hover="#374151",
        button_primary_text_color="white",
        button_secondary_background_fill="white",
        button_secondary_background_fill_hover="#F9FAFB",
        button_secondary_border_color="#E5E7EB",
        button_secondary_text_color="#111827",
        input_background_fill="#FAFAFA",
        block_border_width="0px",
        block_shadow="none",
    )

    with gr.Blocks(
        title="LLM under the hood",
        theme=minimal_theme,
        css="""
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap');

        :root {
            --accent-blue: #2563EB;
            --accent-teal: #14B8A6;
            --surface-blue: rgba(37, 99, 235, 0.04);
            --surface-blue-strong: rgba(37, 99, 235, 0.08);
            --surface-teal: rgba(20, 184, 166, 0.08);
            --attention-color: #F97316;
            --attention-bg: rgba(249, 115, 22, 0.12);
            --softmax-color: #6366F1;
            --softmax-bg: rgba(99, 102, 241, 0.12);
            --layers-color: #0EA5E9;
            --layers-bg: rgba(14, 165, 233, 0.12);
        }

        body, input, textarea, button {
            font-family: 'Inter', system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif !important;
        }

        body {
            font-size: 15px;
            line-height: 1.65;
            color: #111827;
            background: #ffffff;
        }

        p, li {
            line-height: 1.7;
        }

        /* Minimalistic elegant styling */
        .container {
            max-width: 960px;
            margin: 0 auto;
        }

        .title-section {
            padding: 0.5rem 1.25rem 1.1rem 1.25rem;
            margin-bottom: 1.25rem;
            background: linear-gradient(180deg, rgba(37, 99, 235, 0.05) 0%, rgba(255, 255, 255, 0) 100%);
            border-radius: 10px;
        }

        .title-copy {
            flex: 1 1 360px;
            min-width: 300px;
        }

        .title-heading {
            display: flex;
            align-items: baseline;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .title-main {
            font-size: 1.9rem;
            font-weight: 500;
            letter-spacing: -0.02em;
            color: #0f172a;
            margin: 0;
        }

        .usage-label {
            display: inline-block;
            font-size: 0.95rem;
            font-weight: 600;
            color: #1e3a8a;
            background: rgba(30, 58, 138, 0.12);
            border: 1px solid rgba(30, 58, 138, 0.25);
            padding: 0.35rem 0.6rem;
            border-radius: 999px;
            margin-bottom: 0.55rem;
        }

        .title-subtitle {
            font-size: 1rem;
            color: #334155;
            margin: 0.4rem 0 0.85rem 0;
            line-height: 1.6;
            max-width: 36rem;
        }

        .usage-body {
            background: rgba(30, 64, 175, 0.05);
            border: 1px solid rgba(30, 64, 175, 0.1);
            border-radius: 10px;
            padding: 0.9rem 1.1rem;
            font-size: 0.92rem;
            color: #1f2937;
            line-height: 1.65;
        }

        .usage-steps {
            margin: 0;
            padding-left: 1.2rem;
        }

        .usage-steps > li {
            margin-bottom: 0.55rem;
        }

        .usage-steps ul {
            margin-top: 0.4rem;
            padding-left: 1.1rem;
            list-style: disc;
        }

        .usage-steps ul li {
            margin-bottom: 0.25rem;
        }

        .usage-meta {
            margin-top: 0.85rem;
            font-size: 0.85rem;
            color: #475569;
            line-height: 1.6;
        }

        .pipeline-card {
            margin-top: 1.25rem;
            background: linear-gradient(180deg, rgba(14, 165, 233, 0.08) 0%, rgba(255, 255, 255, 0) 100%);
            border-radius: 12px;
            border: 1px solid rgba(14, 165, 233, 0.25);
            padding: 1.1rem 1.25rem;
            box-shadow: 0 4px 12px rgba(14, 165, 233, 0.12);
            max-width: 560px;
        }

        .pipeline-card h4 {
            margin: 0 0 0.75rem 0;
            font-size: 1rem;
            color: #075985;
        }

        .pipeline-card ol {
            margin: 0;
            padding: 0;
            list-style: none;
        }

        .pipeline-step {
            display: flex;
            gap: 0.6rem;
            align-items: flex-start;
            padding: 0.5rem 0;
            border-bottom: 1px solid rgba(14, 165, 233, 0.18);
        }

        .pipeline-step:last-child {
            border-bottom: none;
        }

        .pipeline-check {
            width: 22px;
            height: 22px;
            border-radius: 50%;
            display: inline-flex;
            align-items: center;
            justify-content: center;
            font-size: 0.9rem;
            font-weight: 600;
            border: 1px solid rgba(14, 165, 233, 0.4);
            color: rgba(14, 165, 233, 0.9);
            flex-shrink: 0;
        }

        .pipeline-text {
            flex: 1;
        }

        .pipeline-step-label {
            display: block;
            font-weight: 600;
            font-size: 0.92rem;
            color: #0f172a;
        }

        .pipeline-text p {
            margin: 0.25rem 0 0 0;
            font-size: 0.85rem;
            color: #1f2937;
        }

        .pipeline-step-done .pipeline-check {
            background: rgba(14, 165, 233, 0.15);
        }

        .pipeline-step-active .pipeline-check {
            background: rgba(14, 165, 233, 0.08);
            color: #0284c7;
            border-color: rgba(2, 132, 199, 0.6);
        }

        .pipeline-step-pending .pipeline-check {
            background: rgba(255, 255, 255, 0.8);
            color: rgba(14, 165, 233, 0.4);
            border-style: dashed;
        }

        .pipeline-step-pending .pipeline-step-label {
            color: #1f2937;
        }

        .pipeline-temperature {
            margin: 0.8rem 0 0 0;
            font-size: 0.82rem;
            color: #075985;
        }

        #temperature-note {
            font-size: 0.82rem;
            color: #1d4ed8;
            margin-top: 0.25rem;
        }

        #correction-note {
            font-size: 0.82rem;
            color: #1d4ed8;
            margin: 0.35rem 0 0.65rem 0;
        }

        .prompt-summary {
            text-align: left;
            font-size: 0.9rem;
            color: #1f2937;
            background: rgba(15, 23, 42, 0.03);
            border-radius: 6px;
            padding: 0.85rem 1rem;
            margin-bottom: 1rem;
        }

        .prompt-summary strong {
            color: #0f172a;
        }

        .section-divider {
            height: 1px;
            background: rgba(15, 23, 42, 0.08);
            margin: 1.75rem 0;
        }

        /* Smaller buttons */
        button {
            padding: 0.55rem 1.35rem !important;
            font-size: 0.92rem !important;
            font-weight: 500 !important;
            border-radius: 4px !important;
            letter-spacing: 0.01em;
        }

        button.secondary {
            color: #0f172a !important;
        }

        /* Clean inputs */
        input, textarea {
            border: 1px solid rgba(15, 23, 42, 0.12) !important;
            border-radius: 4px !important;
            font-size: 0.95rem !important;
            background: rgba(255, 255, 255, 0.9) !important;
        }

        /* Theory boxes */
        .theory-box {
            background: linear-gradient(180deg, var(--surface-teal) 0%, rgba(255, 255, 255, 0) 100%);
            border-left: 3px solid var(--accent-teal);
            padding: 1.75rem;
            margin: 1.75rem 0;
            border-radius: 6px;
        }

        .correction-guide {
            background: linear-gradient(180deg, rgba(37, 99, 235, 0.05) 0%, rgba(255, 255, 255, 0) 100%);
            border-radius: 8px;
            border: 1px solid rgba(37, 99, 235, 0.15);
            padding: 1.75rem;
            margin-top: 2.5rem;
        }

        .correction-guide h4 {
            margin: 0 0 0.75rem 0;
            font-size: 1.05rem;
            color: #1f2937;
        }

        .correction-guide ul {
            margin: 0.75rem 0 0 1.25rem;
            color: #334155;
        }

        .comparison-table thead th {
            background: rgba(15, 23, 42, 0.04);
        }

        .comparison-table td {
            background: rgba(255, 255, 255, 0.92);
        }

        .comparison-table tr:not(:last-child) td {
            border-bottom: 1px solid rgba(15, 23, 42, 0.08);
        }

        .soft-note {
            margin-top: 1rem;
            padding: 0.75rem 1rem;
            background: #F9FAFB;
            border-left: 3px solid var(--accent-blue);
            color: #1F2937;
            font-size: 0.9rem;
            border-radius: 4px;
        }

        .section-note {
            padding: 0.85rem 1rem;
            border-radius: 8px;
            font-size: 0.9rem;
            margin-bottom: 1.2rem;
            line-height: 1.6;
        }

        .pill-attention {
            background: var(--attention-bg);
            border-left: 3px solid var(--attention-color);
        }

        .pill-softmax {
            background: var(--softmax-bg);
            border-left: 3px solid var(--softmax-color);
        }

        .pill-layers {
            background: var(--layers-bg);
            border-left: 3px solid var(--layers-color);
        }

        .metric-list {
            display: flex;
            flex-direction: column;
            gap: 0.65rem;
        }

        .metric-row {
            display: grid;
            grid-template-columns: 160px 90px 1fr 60px;
            align-items: center;
            gap: 0.75rem;
            font-size: 0.9rem;
        }

        .metric-label {
            text-align: right;
            font-weight: 500;
            color: #1F2937;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }

        .metric-logit {
            font-family: "JetBrains Mono", "Menlo", monospace;
            font-size: 0.75rem;
            color: #6B7280;
        }

        .metric-secondary {
            font-size: 0.7rem;
            color: #6B7280;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .metric-track {
            position: relative;
            width: 100%;
            height: 6px;
            border-radius: 999px;
            background: rgba(15, 23, 42, 0.08);
            overflow: hidden;
        }

        .metric-fill {
            position: absolute;
            height: 100%;
            left: 0;
            top: 0;
            border-radius: inherit;
        }

        .metric-value {
            font-size: 0.75rem;
            color: #4B5563;
            font-weight: 500;
        }

        .layer-grid {
            display: grid;
            gap: 1rem;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
        }

        .layer-card {
            border: 1px solid rgba(15, 23, 42, 0.1);
            border-radius: 10px;
            padding: 1rem;
            background: #FFFFFF;
            box-shadow: 0 4px 8px rgba(15, 23, 42, 0.04);
        }

        .layer-title {
            font-size: 0.9rem;
            font-weight: 600;
            color: var(--layers-color);
            margin-bottom: 0.5rem;
        }

        .layer-actual {
            font-size: 0.9rem;
            color: #111827;
            margin-bottom: 0.5rem;
        }

        .layer-alternatives {
            margin: 0;
            padding-left: 1rem;
            color: #4B5563;
            font-size: 0.85rem;
        }

        .layer-alternatives li {
            margin-bottom: 0.25rem;
        }

        .comparison-card {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
            gap: 1rem;
        }

        .comparison-card .column {
            border: 1px solid rgba(15, 23, 42, 0.1);
            border-radius: 10px;
            padding: 1rem;
            background: #FFFFFF;
        }

        .info-tooltip {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7rem;
            background: rgba(15, 23, 42, 0.08);
            color: #1f2937;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            margin-left: 0.3rem;
            cursor: help;
            position: relative;
        }

        .info-tooltip::after {
            content: attr(data-tip);
            position: absolute;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            background: #111827;
            color: #f8fafc;
            padding: 0.4rem 0.6rem;
            font-size: 0.75rem;
            border-radius: 4px;
            opacity: 0;
            pointer-events: none;
            white-space: nowrap;
            transition: opacity 0.15s ease;
            box-shadow: 0 4px 12px rgba(15, 23, 42, 0.18);
        }

        .info-tooltip:hover::after {
            opacity: 1;
        }

        .heatmap-wrapper, .waterfall-wrapper, .sparkline-wrapper {
            margin-top: 1.25rem;
            background: rgba(15, 23, 42, 0.02);
            border: 1px solid rgba(15, 23, 42, 0.08);
            border-radius: 10px;
            padding: 1rem 1.2rem;
        }

        .heatmap-heading, .waterfall-heading, .sparkline-heading {
            font-size: 0.9rem;
            color: #1f2937;
            margin-bottom: 0.75rem;
            font-weight: 600;
        }

        .waterfall-note {
            margin: 0.35rem 0 0.75rem 0;
            font-size: 0.78rem;
            color: #4338ca;
        }

        .heatmap-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.82rem;
            background: white;
            border-radius: 8px;
            overflow: hidden;
        }

        .heatmap-table th,
        .heatmap-table td {
            border: 1px solid rgba(148, 163, 184, 0.25);
            padding: 0.4rem 0.5rem;
            text-align: center;
        }

        .heatmap-table th {
            background: rgba(249, 115, 22, 0.1);
            color: #9a3412;
            font-weight: 600;
        }

        .heatmap-table th:first-child {
            text-align: left;
            min-width: 140px;
        }

        .heatmap-source {
            display: inline-block;
            margin-right: 0.45rem;
            padding: 0.1rem 0.35rem;
            border-radius: 999px;
            font-size: 0.7rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: #7c2d12;
            background: rgba(249, 115, 22, 0.15);
        }

        .heatmap-source-question,
        .heatmap-source-prompt {
            color: #1e3a8a;
            background: rgba(30, 58, 138, 0.1);
        }

        .heatmap-source-context {
            color: #166534;
            background: rgba(22, 101, 52, 0.12);
        }

        .heatmap-value {
            font-weight: 500;
            color: #7c2d12;
        }

        .waterfall-row {
            display: grid;
            grid-template-columns: 1.4fr 1fr 1.8fr 1.2fr;
            gap: 0.75rem;
            align-items: center;
            padding: 0.55rem 0.35rem;
            border-bottom: 1px solid rgba(148, 163, 184, 0.2);
        }

        .waterfall-row:last-child {
            border-bottom: none;
        }

        .waterfall-token {
            font-weight: 600;
            color: #312e81;
        }

        .waterfall-stage {
            font-size: 0.78rem;
            color: #334155;
            display: flex;
            align-items: center;
            gap: 0.45rem;
        }

        .waterfall-bar {
            position: relative;
            height: 8px;
            border-radius: 999px;
            flex: 0 0 90px;
        }

        .waterfall-bar.exp {
            background: linear-gradient(90deg, rgba(99, 102, 241, 0.15), rgba(99, 102, 241, 0.6));
        }

        .waterfall-bar.prob {
            background: linear-gradient(90deg, rgba(99, 102, 241, 0.2), rgba(99, 102, 241, 0.8));
        }

        .sparkline {
            width: 100%;
            height: auto;
        }

        .sparkline-area {
            fill: rgba(14, 165, 233, 0.15);
        }

        .sparkline-line {
            fill: none;
            stroke: rgba(14, 165, 233, 0.9);
            stroke-width: 2;
        }

        .sparkline-axis-line {
            stroke: #94a3b8;
            stroke-width: 1;
        }

        .sparkline-tick-mark {
            stroke: #cbd5f5;
            stroke-width: 1;
        }

        .sparkline-tick-label {
            font-size: 0.55rem;
            fill: #475569;
            text-anchor: middle;
            dominant-baseline: hanging;
        }

        [data-testid="status"] {
            display: none !important;
        }

        .sparkline-axis-caption {
            font-size: 0.7rem;
            fill: #475569;
            text-anchor: end;
            font-weight: 600;
        }

        .delta-wrapper {
            margin-top: 1.5rem;
            background: rgba(15, 23, 42, 0.02);
            border: 1px solid rgba(15, 23, 42, 0.08);
            border-radius: 10px;
            padding: 1.2rem 1.4rem;
        }

        .delta-heading {
            font-size: 0.95rem;
            font-weight: 600;
            color: #0f172a;
            margin-bottom: 0.9rem;
        }

        .delta-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 1rem;
        }

        .delta-card {
            background: #fff;
            border-radius: 8px;
            border: 1px solid rgba(148, 163, 184, 0.25);
            padding: 0.9rem 1rem;
            box-shadow: 0 4px 6px rgba(15, 23, 42, 0.04);
        }

        .delta-card h4 {
            margin: 0 0 0.6rem 0;
            font-size: 0.9rem;
            color: #1f2937;
        }

        .delta-card ul {
            margin: 0;
            padding-left: 1rem;
            color: #334155;
            font-size: 0.8rem;
        }

        .delta-label {
            font-weight: 600;
            color: #111827;
        }

        .delta-value {
            margin-left: 0.4rem;
            font-family: "JetBrains Mono", "Menlo", monospace;
            color: #1d4ed8;
        }
        """
    ) as demo:

        with gr.Column(elem_id="title-section"):
            gr.HTML(
                """
                <div class="title-copy">
                    <div class="title-heading">
                        <h1 class="title-main">LLM under the hood</h1>
                    </div>
                    <p class="title-subtitle">Watch how a compact language model reads your question, weighs each word, and decides on its next token.</p>
                    <div class="usage-body">
                        <span class="usage-label">How to use this app</span>
                        <ol class="usage-steps">
                            <li>Ask a crisp factual question ‚Äî single-word answers make the internals easiest to read.</li>
                            <li>Adjust the <strong>Temperature</strong> slider (right under the question box) to compare steady vs exploratory behaviour before you run.</li>
                            <li>Scan the Query tabs from left to right to connect the attention bars, heatmap, softmax waterfall, and confidence sparkline to the answer.</li>
                            <li>After the first try choose one feedback path:
                                <ul>
                                    <li><strong>That's Wrong</strong> tells TinyLlama to retry while keeping your question the same.</li>
                                    <li><strong>Submit Correction</strong> sends the exact word you expect so the next turn can aim directly at it.</li>
                                </ul>
                            </li>
                        </ol>
                    </div>
                    <p class="usage-meta"><strong>Model:</strong> TinyLlama-1.1B is an open chat transformer with 1.1&nbsp;billion learned parameters. ChatGPT‚Äôs GPT-4 is tens of times larger and more capable, but this lightweight model lets us reveal every attention map, hidden layer, and probability calculation in real time.</p>
                </div>
                """,
                elem_id="title-copy",
            )
            pipeline_progress = gr.HTML(
                value=_build_pipeline_html(0, None, DEFAULT_TEMPERATURE),
                visible=True,
                elem_id="pipeline-card",
            )

        gr.HTML('<div class="section-divider"></div>')

        empty_attention_html = "<em>Run the model to list attention percentages for every word in your prompt and context.</em>"
        empty_attention_heatmap = "<em>Run the model to visualise the full attention heatmap.</em>"
        empty_softmax_html = "<em>Run the model to reveal the top candidate words and their probabilities.</em>"
        empty_softmax_waterfall = "<em>Run the model to watch logits turn into probabilities.</em>"
        empty_layers_html = "<em>Run the model to chart how confidence evolves across layers.</em>"
        empty_layers_sparkline = "<em>Run the model to trace the confidence sparkline.</em>"
        correction_placeholder = "_Generate an answer and press ‚ÄúThat‚Äôs Wrong‚Äù to compare before and after._"

        with gr.Tabs():
            with gr.Tab("Query"):
                gr.Markdown("### Ask the model a quick fact")
                gr.Markdown(
                    "Use this tab to follow the model's first attempt. Short prompts such as **What is the capital of Australia?** work best because the answer should be a single word. Want the full story? üìò [Read the overview theory](#theory-overview)."
                )

                question_input = gr.Textbox(
                    label="Question",
                    placeholder="What is the capital of Australia?",
                    lines=1,
                    show_label=True,
                    container=False
                )
                temperature_slider = gr.Slider(
                    label="Temperature",
                    minimum=0.1,
                    maximum=1.5,
                    step=0.05,
                    value=DEFAULT_TEMPERATURE,
                    info="Lower = confident & deterministic ¬∑ Higher = exploratory & diverse",
                    interactive=True,
                )

                gr.Markdown(
                    "üí° **Temperature tip:** This slider rescales the logits before softmax. Cool settings (‚âà0.1‚Äì0.3) make the top token dominate, while warmer settings (‚âà0.8‚Äì1.2) keep more candidates alive so you can see probabilities redistribute.",
                    elem_id="temperature-note",
                )

                generate_btn = gr.Button("Generate", variant="primary", size="sm")

                response_output = gr.Textbox(
                    label="Model's Answer",
                    lines=1,
                    interactive=False
                )

                query_overview_md = gr.Markdown(
                    "_Run the model to see how it read your prompt, picked a candidate, and justified the choice._"
                )

                with gr.Tabs():
                    with gr.Tab("üü† Attention"):
                        gr.Markdown(
                            "**Attention** is how the model decides which prompt words to re-read before speaking. The orange bars total the attention percentage for each prompt or context word when choosing the highlighted answer, and the heatmap spreads the very same percentages across every generated token. üìò [Open the attention theory](#theory-attention)."
                        )
                        query_attention_panel = gr.HTML(empty_attention_html)
                        query_attention_heatmap = gr.HTML(empty_attention_heatmap)

                    with gr.Tab("üü£ Softmax"):
                        gr.Markdown(
                            "**Softmax** is the final conversion from raw scores to real probabilities. The purple list shows the top candidates after normalisation, while the waterfall tracks how each token‚Äôs logit grows into an exponential and then a probability. üìò [Open the softmax theory](#theory-softmax)."
                        )
                        query_softmax_panel = gr.HTML(empty_softmax_html)
                        query_softmax_waterfall = gr.HTML(empty_softmax_waterfall)

                    with gr.Tab("üîµ Layer by Layer"):
                        gr.Markdown(
                            "Transformer **layers** are stacked reasoning steps. The blue cards sample the top alternatives after each layer‚Äôs update, and the sparkline plots how confident the model becomes in the final answer as the layer number increases. üìò [Open the layer theory](#theory-layers)."
                        )
                        query_layers_panel = gr.HTML(empty_layers_html)
                        query_layers_sparkline = gr.HTML(empty_layers_sparkline)

            with gr.Tab("Correction"):
                gr.Markdown("### Teach the model when it slips")
                gr.Markdown(
                    "Challenge the answer and optionally provide the correct word. The comparison panels show exactly how the internal signals respond to your feedback. üìò [See why feedback matters](#theory-feedback)."
                )

                wrong_btn = gr.Button("That's Wrong", variant="stop", size="sm")

                gr.Markdown("**Optional:** Type the correct word so the model can aim directly at it.")
                correction_input = gr.Textbox(
                    label="",
                    placeholder="Type the word it should have said",
                    lines=1,
                    show_label=False
                )

                correction_btn = gr.Button("Submit Correction", variant="secondary", size="sm")

                gr.Markdown(
                    "_Choose one path per turn: use **That's Wrong** when you just want the model to try again with a gentle nudge, or provide the exact word and press **Submit Correction** to steer it directly._",
                    elem_id="correction-note",
                )

                correction_summary_md = gr.Markdown(correction_placeholder)

                with gr.Tabs():
                    with gr.Tab("üü† Attention"):
                        gr.Markdown("See how the attention mechanism reweights your prompt or context words after feedback. The twin bar charts and heatmaps show the exact percentage shifts before and after your hint. üìò [Revisit the attention theory](#theory-attention).")
                        correction_attention_panel = gr.HTML("<em>No comparison yet.</em>")

                    with gr.Tab("üü£ Softmax"):
                        gr.Markdown("Check whether the corrected word overtakes its competitors once softmax recomputes the probabilities with your hint in mind. üìò [Revisit the softmax theory](#theory-softmax).")
                        correction_softmax_panel = gr.HTML("<em>No comparison yet.</em>")

                    with gr.Tab("üîµ Layer by Layer"):
                        gr.Markdown("Look for deeper layers to embrace the corrected answer sooner or with greater certainty. The cards and sparkline reveal where in the stack the change really takes hold. üìò [Revisit the layer theory](#theory-layers).")
                        correction_layers_panel = gr.HTML("<em>No comparison yet.</em>")

                    with gr.Tab("Œî Impact"):
                        gr.Markdown("Read this tab for the biggest attention, probability, and layer shifts triggered by your feedback. üìò [Review the feedback theory](#theory-feedback).")
                        correction_delta_panel = gr.HTML("<em>No comparison yet.</em>")

            with gr.Tab("Theory"):
                gr.Markdown("### Peek behind the math")
                gr.Markdown(
                    "These notes explain the coloured panels in everyday language before linking them back to the underlying equations."
                )

                theory_overview_md = gr.Markdown(THEORY_TEXT["overview"], elem_id="theory-overview")
                theory_feedback_md = gr.Markdown(THEORY_TEXT["feedback"], elem_id="theory-feedback")

                with gr.Tabs():
                    with gr.Tab("üü† Attention"):
                        theory_attention_md = gr.Markdown(THEORY_TEXT["attention"], elem_id="theory-attention")

                    with gr.Tab("üü£ Softmax"):
                        theory_softmax_md = gr.Markdown(THEORY_TEXT["softmax"], elem_id="theory-softmax")

                    with gr.Tab("üîµ Layer by Layer"):
                        theory_layers_md = gr.Markdown(THEORY_TEXT["layers"], elem_id="theory-layers")

        # Event handlers
        # Store original answer to avoid re-generation
        original_answer_cache: Dict[Tuple[str, float], Dict[str, object]] = {}

        def _make_cache_key(question: str, temperature: float) -> Tuple[str, float]:
            return (question.strip(), round(float(temperature), 3))

        def _build_idle_pipeline(temperature: float):
            return gr.update(
                value=_build_pipeline_html(0, None, temperature),
                visible=True,
            )

        def _progress_stub() -> List[gr.Update]:
            return [gr.update() for _ in range(19)]

        def on_temperature_change(temperature: float):
            return _build_idle_pipeline(temperature)

        def on_generate(question, temperature):
            """Generate answer and update all tabs with visible pipeline progress."""

            stripped = (question or "").strip()
            if not stripped:
                yield (
                    "Please enter a question!",
                    "_Ask something like ‚ÄúWhat is the capital of Australia?‚Äù_",
                    _build_idle_pipeline(temperature),
                    empty_attention_html,
                    empty_attention_html,
                    empty_attention_heatmap,
                    empty_softmax_html,
                    empty_softmax_waterfall,
                    empty_layers_html,
                    empty_layers_sparkline,
                    THEORY_TEXT["overview"],
                    THEORY_TEXT["feedback"],
                    THEORY_TEXT["attention"],
                    THEORY_TEXT["softmax"],
                    THEORY_TEXT["layers"],
                    correction_placeholder,
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            stage_one = _progress_stub()
            stage_one[1] = gr.update(value="‚è≥ TinyLlama is tokenising your question‚Ä¶")
            stage_one[2] = gr.update(value=_build_pipeline_html(1, 2, temperature))
            yield tuple(stage_one)

            time.sleep(0.12)

            stage_two = _progress_stub()
            stage_two[1] = gr.update(value="‚è≥ Sharing context through attention‚Ä¶")
            stage_two[2] = gr.update(value=_build_pipeline_html(2, 3, temperature))
            yield tuple(stage_two)

            time.sleep(0.12)

            stage_three = _progress_stub()
            stage_three[1] = gr.update(value="‚è≥ Crunching layers and scoring candidates‚Ä¶")
            stage_three[2] = gr.update(value=_build_pipeline_html(3, 4, temperature))
            yield tuple(stage_three)

            answer, payload = generate_one_word_answer(question, temperature=temperature)

            if isinstance(payload, str):
                yield (
                    answer,
                    payload,
                    _build_idle_pipeline(temperature),
                    empty_attention_html,
                    empty_attention_heatmap,
                    empty_softmax_html,
                    empty_softmax_waterfall,
                    empty_layers_html,
                    empty_layers_sparkline,
                    THEORY_TEXT["overview"],
                    THEORY_TEXT["feedback"],
                    THEORY_TEXT["attention"],
                    THEORY_TEXT["softmax"],
                    THEORY_TEXT["layers"],
                    correction_placeholder,
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            pipeline_update = gr.update(
                value=_build_pipeline_html(len(PIPELINE_STEPS), None, temperature),
                visible=True,
            )

            cache_key = _make_cache_key(question, temperature)
            original_answer_cache[cache_key] = {
                "answer": answer,
                "payload": payload,
            }

            yield (
                answer,
                payload["overview"],
                pipeline_update,
                payload["attention"]["markdown"],
                payload["attention"]["heatmap"],
                payload["softmax"]["markdown"],
                payload["softmax"]["waterfall"],
                payload["layers"]["markdown"],
                payload["layers"]["sparkline"],
                payload["theory"]["overview"],
                payload["theory"]["feedback"],
                payload["theory"]["attention"],
                payload["theory"]["softmax"],
                payload["theory"]["layers"],
                correction_placeholder,
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
            )

        def on_wrong_clicked(question, temperature):
            stripped_question = (question or "").strip()
            cache_key = _make_cache_key(stripped_question, temperature)
            entry = original_answer_cache.get(cache_key)
            if entry is None and stripped_question:
                entry = next(
                    (
                        payload
                        for (stored_question, _), payload in original_answer_cache.items()
                        if stored_question == stripped_question
                    ),
                    None,
                )
            if not stripped_question or entry is None:
                yield (
                    _build_idle_pipeline(temperature),
                    "‚ö†Ô∏è Please generate an answer first!",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            original_entry = entry
            original_answer = original_entry["answer"]
            original_payload = original_entry["payload"]

            correction_context = f"{original_answer} is wrong, the right answer is"
            corrected_answer, corrected_payload = generate_one_word_answer(
                question,
                context=correction_context,
                temperature=temperature,
            )

            if isinstance(corrected_payload, str):
                yield (
                    _build_idle_pipeline(temperature),
                    corrected_payload,
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            stage_one = gr.update(value=_build_pipeline_html(1, 2, temperature), visible=True)
            yield (
                stage_one,
                "_We‚Äôre preparing the correction run‚Ä¶_",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
            )

            time.sleep(0.12)

            stage_two = gr.update(value=_build_pipeline_html(3, 4, temperature), visible=True)
            yield (
                stage_two,
                "_Recomputing attention, layers, and probabilities‚Ä¶_",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
            )

            summary, attention_html, softmax_html, layers_html, delta_html = build_correction_sections(
                question,
                original_answer,
                original_payload,
                correction_context,
                corrected_answer,
                corrected_payload,
            )

            pipeline_update = gr.update(
                value=_build_pipeline_html(len(PIPELINE_STEPS), None, temperature),
                visible=True,
            )

            yield pipeline_update, summary, attention_html, softmax_html, layers_html, delta_html

        def on_correction(question, correction, temperature):
            if not correction.strip():
                yield (
                    _build_idle_pipeline(temperature),
                    "‚ö†Ô∏è Please enter a correction!",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            stripped_question = (question or "").strip()
            cache_key = _make_cache_key(stripped_question, temperature)
            entry = original_answer_cache.get(cache_key)
            if entry is None and stripped_question:
                entry = next(
                    (
                        payload
                        for (stored_question, _), payload in original_answer_cache.items()
                        if stored_question == stripped_question
                    ),
                    None,
                )
            if not stripped_question or entry is None:
                yield (
                    _build_idle_pipeline(temperature),
                    "‚ö†Ô∏è Please generate an answer first!",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            original_entry = entry
            original_answer = original_entry["answer"]
            original_payload = original_entry["payload"]

            correction_context = f"{original_answer} is wrong, the right answer is {correction.strip()}"
            corrected_answer, corrected_payload = generate_one_word_answer(
                question,
                context=correction_context,
                temperature=temperature,
            )

            if isinstance(corrected_payload, str):
                yield (
                    _build_idle_pipeline(temperature),
                    corrected_payload,
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                    "<em>No comparison yet.</em>",
                )
                return

            stage_one = gr.update(value=_build_pipeline_html(1, 2, temperature), visible=True)
            yield (
                stage_one,
                "_Blending your correction into the prompt‚Ä¶_",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
            )

            time.sleep(0.12)

            stage_two = gr.update(value=_build_pipeline_html(3, 4, temperature), visible=True)
            yield (
                stage_two,
                "_Running TinyLlama again with your hint‚Ä¶_",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
                "<em>No comparison yet.</em>",
            )

            summary, attention_html, softmax_html, layers_html, delta_html = build_correction_sections(
                question,
                original_answer,
                original_payload,
                correction_context,
                corrected_answer,
                corrected_payload,
            )

            pipeline_update = gr.update(
                value=_build_pipeline_html(len(PIPELINE_STEPS), None, temperature),
                visible=True,
            )

            yield pipeline_update, summary, attention_html, softmax_html, layers_html, delta_html
        generate_btn.click(
            fn=on_generate,
            inputs=[question_input, temperature_slider],
            outputs=[
                response_output,
                query_overview_md,
                pipeline_progress,
                query_attention_panel,
                query_attention_heatmap,
                query_softmax_panel,
                query_softmax_waterfall,
                query_layers_panel,
                query_layers_sparkline,
                theory_overview_md,
                theory_feedback_md,
                theory_attention_md,
                theory_softmax_md,
                theory_layers_md,
                correction_summary_md,
                correction_attention_panel,
                correction_softmax_panel,
                correction_layers_panel,
                correction_delta_panel,
            ],
            show_progress=False
        )

        wrong_btn.click(
            fn=on_wrong_clicked,
            inputs=[question_input, temperature_slider],
            outputs=[
                pipeline_progress,
                correction_summary_md,
                correction_attention_panel,
                correction_softmax_panel,
                correction_layers_panel,
                correction_delta_panel,
            ],
            show_progress=False
        )

        correction_btn.click(
            fn=on_correction,
            inputs=[question_input, correction_input, temperature_slider],
            outputs=[
                pipeline_progress,
                correction_summary_md,
                correction_attention_panel,
                correction_softmax_panel,
                correction_layers_panel,
                correction_delta_panel,
            ],
            show_progress=False
        )

        temperature_slider.change(
            fn=on_temperature_change,
            inputs=[temperature_slider],
            outputs=[pipeline_progress],
            queue=False,
        )
        return demo


if __name__ == "__main__":
    demo = main_interface()
    demo.queue()  # Enable queuing for concurrent users
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,  # HF Spaces default
        share=False
    )
